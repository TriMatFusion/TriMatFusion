# TriMatFusion
This guide will walk you through the complete pipeline for training the tri-modal fusion model, from data collection to analyzing the final results.

The model leverages three distinct modalities:
1.  **Graph Embeddings** (from GNNs like CGCNN)
2.  **Text Embeddings** (from MatSciBERT on `robocrys` descriptions)
3.  **Elemental Fingerprints** (custom physical attributes)

Let's get started.

## 1. Data Collection

Before you can run the model, you need to prepare your data in a specific format. Your data directory (e.g., `./data/`) should contain two components:

### Structure Files

These are the crystal structure files for your materials.
* **Source:** They can be your own DFT calculations or downloaded from open-source datasets (e.g., Materials Project, JARVIS, OBLiX, AdsMT).
* **Format:** The model uses **Atomic Simulation Environment ([ASE](https://ase-lib.org/))** to read these files. Any format supported by ASE is acceptable, including `.cif`, `POSCAR`, `ASE JSON`, etc.

### Target Properties File (`id_prop.csv`)

This is the main file that links your structures to their target properties.
* **Name:** It must be named `id_prop.csv`.
* **Location:** It should be placed in the root of your data directory (e.g., `./data/id_prop.csv`).
* **Content:** It must contain at least two columns:
    1.  `cif`: The exact filename of the corresponding structure file (e.g., `sid63.cif`).
    2.  `target`: The numerical property you want to predict (e.g., adsorption energy, ionic conductivity, band gap).
    
    > **Tip:** If your target property spans several orders of magnitude (e.g., electrical conductivity), it is recommended to apply a logarithmic transformation to the target values and document the units/transformation in your dataset.

A typical data directory might look like this:
```
/data/
├── id_prop.csv
├── sid63.cif
├── sid181.cif
└── ... (all other structure files)
```

## 2. Pre-processing: Generate Text Descriptions

The second modality, text descriptions, is generated by converting your crystal structures into natural language descriptions. This process is handled using **[Robocrystallographer](https://hackingmaterials.lbl.gov/robocrystallographer/)**.

Specifically, `robocrys` reads your structure files (e.g., from `id_prop.csv`), transforms them into detailed textual descriptions, and then these descriptions, along with the original CIF IDs and targets, are compiled into a new `text.csv` file.

After this step, your data directory will contain the `text.csv` file:

```
/data/
├── id_prop.csv
├── text.csv  <-- NEWLY CREATED
├── sid63.cif
└── ...
```

The `text.csv` file will have the following columns: `["cif", "description", "target"]`.

## 3. Main Data Processing

This is the most important step. Here, the code will:
1.  Read all data (structures, `text.csv`).
2.  Generate the Graph (GNN) modality.
3.  Generate the Fingerprint modality.
4.  Tokenize the Text modality.
5.  Combine all three modalities into a single PyTorch Geometric (PyG) dataset file.

### **ACTION REQUIRED: Modify `dataset.py` for your Fingerprint**

> **Warning:** The `fingerprint` processing logic is **system-specific**. Our code is designed for doped CuS systems. You **must** modify the fingerprint generation section in `dataset.py` to match the attributes of your specific chemical system.

After ensuring `dataset.py` is configured for your system, the data will be processed **automatically when you first run `python main.py`**.

This initial run will create a new `processed/` directory inside your `data_path`, containing two key files:

* `data.pt`: A single file containing the entire processed PyG dataset. It includes all attributes like `data.edge_attr`, `data.fingerprint`, `data.text_input_ids`, `data.y` (target),  etc.
* `split_datasets.pkl`: This file stores the fixed train/validation/test indices. This ensures that all subsequent training runs use the exact same data splits for fair and reproducible results.

**Note:** If your data changes or you modify `dataset.py`, you'll need to manually delete the `processed/` directory and `split_datasets.pkl` files to force a re-processing the next time you run `python main.py`.

## 4. Configure and Run Training

Now you are ready to train!

### Configure `config.yml`

All model hyperparameters, training settings, and file paths are controlled by the `config.yml` file. Open it and set your desired parameters:

```yaml
# config.yml (Example)
Job:
  job_name: "my_train_job"
  seed: 42
  # ...

Processing:
  data_path: "./data/"
  target_path: "text.csv"
  # ...

Training:
  train_ratio: 0.8
  val_ratio: 0.05
  test_ratio: 0.15
  # ...

Models:
  dropout_rate: 0.4
  epochs: 300
  lr: 0.0003
  batch_size: 24
  optimizer_args: {"weight_decay": 0.0005}
  # ...
```

### Start Training


```bash
python main.py 
```

The script will now use the pre-processed `data.pt` and `split_datasets.pkl` files to train the model.

## 5. Get Results and Visualize

After the training loop is complete, the script will output two main things:

1.  **The Best Model:** A file named `my_model.pth` (or whatever you specified in `job_name` and `model_path`) containing the model weights that achieved the best validation score.
2.  **Output CSVs:** Several CSV files detailing the model's performance on each data split (e.g., `my_train_job_test_outputs.csv`).

For a single-target regression task, the output CSVs will look like this:

| ids | target_1 | prediction_1 |
| :--- | :--- | :--- |
| sid63 | -5.4701924 | -5.96797 |
| sid181 | -4.4243374 | -3.9483082 |
| ... | ... | ... |

You can easily analyze these results using libraries like `pandas`. (You can paste the code below into a **code cell**):

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score

# Load test results
results = pd.read_csv("my_train_job_test_outputs.csv")

# Check R-squared
r2 = r2_score(results['target_1'], results['prediction_1'])
print(f"Test R-squared: {r2:.4f}")

# Plot
plt.scatter(results['target_1'], results['prediction_1'], alpha=0.5)
plt.plot([results['target_1'].min(), results['target_1'].max()], 
         [results['target_1'].min(), results['target_1'].max()], 
         'r--') # Add y=x line
plt.xlabel("True Target")
plt.ylabel("Predicted Target")
plt.title("Test Set Performance")
plt.show()
```

### Attention Visualization

To analyze the model's interpretability (i.e., which words the model "focused on" in the text modality), we use the **[Attention Visualizer](https://github.com/AlaFalaki/AttentionVisualizer/tree/main?tab=readme-ov-file)** tool.
